"""Experimaestro launcher configuration.

Memory-based token system for local task execution.
Generated by: experimaestro launchers direct generate

Token configuration:
{%- if use_memory_tokens %}
  - Memory: {{ total_memory_tokens }} tokens x {{ memory_token_unit_mib }} MiB = {{ total_memory_tokens * memory_token_unit_mib }} MiB
{%- endif %}
{%- if use_gpu_tokens and total_gpu_tokens > 0 %}
{%- if gpu_mode == "exclusive" %}
  - GPU: {{ total_gpu_tokens }} tokens (exclusive access)
{%- else %}
  - GPU: {{ total_gpu_tokens }} tokens x {{ gpu_token_unit_mib }} MiB
{%- endif %}
{%- endif %}
{%- if is_mps and mps_shared_memory %}
  - Note: Apple MPS uses unified memory (GPU shares CPU RAM)
{%- endif %}
"""

from typing import Set
from experimaestro.launcherfinder import (
    HostRequirement,
    HostSpecification,
    CPUSpecification,
{%- if has_cuda_gpus %}
    CudaSpecification,
{%- endif %}
{%- if is_mps %}
    MPSSpecification,
{%- endif %}
)
from experimaestro.launchers.direct import DirectLauncher
from experimaestro.connectors.local import LocalConnector
from experimaestro.launcherfinder.registry import LauncherRegistry


# Token unit sizes (bytes)
MEMORY_TOKEN_UNIT = {{ memory_token_unit_mib }} * (1024 ** 2)  # {{ memory_token_unit_mib }} MiB
{%- if use_gpu_tokens and gpu_mode == "memory" %}
GPU_TOKEN_UNIT = {{ gpu_token_unit_mib }} * (1024 ** 2)  # {{ gpu_token_unit_mib }} MiB
{%- endif %}

# System limits
MAX_MEMORY_TOKENS = {{ total_memory_tokens }}
MAX_GPU_TOKENS = {{ total_gpu_tokens }}
IS_MPS = {{ is_mps }}  # Apple Silicon unified memory


def _calculate_memory_tokens(memory_bytes: int) -> int:
    """Calculate tokens needed for a memory requirement."""
    if memory_bytes <= 0:
        return 0
    return max(1, (memory_bytes + MEMORY_TOKEN_UNIT - 1) // MEMORY_TOKEN_UNIT)

{%- if use_gpu_tokens and gpu_mode == "memory" and not is_mps %}


def _calculate_gpu_tokens(memory_bytes: int) -> int:
    """Calculate GPU tokens needed for GPU memory requirement."""
    if memory_bytes <= 0:
        return 0
    return max(1, (memory_bytes + GPU_TOKEN_UNIT - 1) // GPU_TOKEN_UNIT)
{%- endif %}


def find_launcher(requirements: HostRequirement, tags: Set[str] = set()):
    """Find a launcher for the given requirements.

    Args:
        requirements: Host requirements (CPU memory, GPU memory, etc.)
        tags: Optional tags to filter launchers

    Returns:
        A Launcher instance, or None if requirements cannot be met
    """
    registry = LauncherRegistry.instance()
    connector = LocalConnector.instance()

    # Build host specification for matching
    spec = HostSpecification(
        cpu=CPUSpecification(memory={{ available_memory }}, cores={{ cpu_count }}),
{%- if has_cuda_gpus %}
        accelerators=[{% for gpu in gpus %}CudaSpecification(memory={{ gpu.memory_bytes }}){{ ", " if not loop.last else "" }}{% endfor %}],
{%- elif is_mps %}
        accelerators=[MPSSpecification(memory={{ available_memory }})],  # MPS unified
{%- else %}
        accelerators=[],
{%- endif %}
    )

    # Check if requirements can be satisfied
    match = requirements.match(spec)
    if not match:
        return None  # Requirements exceed system capabilities

    # Calculate token requirements
    req = match.requirement
    memory_tokens = 0
    gpu_tokens = 0
{% if use_memory_tokens %}
    # CPU memory tokens
    if req.cpu and req.cpu.memory:
        memory_tokens = _calculate_memory_tokens(req.cpu.memory)
        if memory_tokens > MAX_MEMORY_TOKENS:
            return None  # Exceeds available memory
{% endif %}
{%- if use_gpu_tokens %}
{%- if is_mps and mps_shared_memory %}
    # GPU memory (MPS: unified memory with CPU)
    if req.accelerators:
        for gpu in req.accelerators:
            if gpu.memory:
                # MPS: GPU memory is shared with CPU
                gpu_mem_tokens = _calculate_memory_tokens(gpu.memory)
                memory_tokens += gpu_mem_tokens
        if memory_tokens > MAX_MEMORY_TOKENS:
            return None  # Total memory (CPU+GPU) exceeds available
{%- elif gpu_mode == "exclusive" %}
    # GPU tokens (exclusive mode)
    if req.accelerators:
        gpu_tokens = len(req.accelerators)
        if gpu_tokens > MAX_GPU_TOKENS:
            return None  # Not enough GPUs
{%- else %}
    # GPU memory tokens
    if req.accelerators:
        for gpu in req.accelerators:
            if gpu.memory:
                gpu_tokens += _calculate_gpu_tokens(gpu.memory)
        if gpu_tokens > MAX_GPU_TOKENS:
            return None  # Exceeds GPU memory
{%- endif %}
{% endif %}
    # Create launcher with token dependencies
    launcher = DirectLauncher(connector=connector)
{% if use_memory_tokens %}
    if memory_tokens > 0:
        memory_token = registry.getToken("memory")
        launcher.addListener(lambda job: job.dependencies.add(memory_token.dependency(memory_tokens)))
{% endif %}
{%- if use_gpu_tokens and total_gpu_tokens > 0 %}
    if gpu_tokens > 0:
        gpu_token = registry.getToken("gpu")
        launcher.addListener(lambda job: job.dependencies.add(gpu_token.dependency(gpu_tokens)))
{% endif %}
    return launcher
